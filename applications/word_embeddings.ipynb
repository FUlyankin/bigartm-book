{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic word embeddings\n",
    "\n",
    "## Anna Potapenko, Artem Popov, Konstantin Vorontsov\n",
    "\n",
    "This is an example of Probabilistic Word Embeddings construction. You can read about Probabilistic Word Embeddings in our paper \"Interpretable probabilistic embeddings: bridging the gap between topic models and neural networks\", AINL-2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparing the collection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Omer Levy's script hyperword: https://bitbucket.org/omerlevy/hyperwords\n",
    "\n",
    "This repository contains tools for transforming document collection to a list of words pairs and their cooccurences. \n",
    "Some hyperparameters are usually pre-set or obscured in other packages and hyperword script allows to tune them all manually. \n",
    "\n",
    "On the Ubuntu system you can create .sh file with required commands. Our file is looking like this:\n",
    "\n",
    "```\n",
    "CORPUS=enwiki_clean.txt\n",
    "mkdir w5.sub\n",
    "\n",
    "python hyperwords/my_corpus2pairs.py --win 5 --sub 1e-5 ${CORPUS} > w5/pairs\n",
    "scripts/pairs2counts.sh w5/pairs > w5/counts \n",
    "python hyperwords/counts2vocab.py w5/counts\n",
    "\n",
    "```\n",
    "\n",
    "Than you have to create vowpal wabbit file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pseudo_docs = defaultdict(str)\n",
    "\n",
    "with open('w5/counts') as f:\n",
    "    for line in f:\n",
    "        count, word1, word2 = line.strip().split()\n",
    "        pseudo_docs[word1] += word2 + ':' + str(count) + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vowpal wabbit file with the collection\n",
    "output = open('vw_coocurence_counts', 'w')\n",
    "\n",
    "for key in sorted(pseudo_docs.keys()):\n",
    "    s = key + ' |  ' + pseudo_docs[key] + '\\n'\n",
    "    output.write(s)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training topic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s learn topic model on the coocurence collection using BigARTM library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import artm\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we convert data to the BigARTM batches format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vowpal wabbit file with the collection\n",
    "file_name = 'vw_coocurence_counts'\n",
    "\n",
    "# folder with batches\n",
    "batches_path = 'batches'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if len(glob.glob(batches_path + \"/*.batch\")) < 1:\n",
    "    batch_vectorizer = artm.BatchVectorizer(data_path=file_name, \n",
    "                                            data_format='vowpal_wabbit', batch_size = 1000, \n",
    "                                            target_folder=batches_path)\n",
    "else:\n",
    "    batch_vectorizer = artm.BatchVectorizer(data_path=batches_path, \n",
    "                                            data_format='batches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dictionary to initialize the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_dictionary = artm.Dictionary()\n",
    "\n",
    "if len(glob.glob(batches_path + \"/*.dict\")) < 1:\n",
    "    my_dictionary.gather(data_path=batches_path)\n",
    "    my_dictionary.save(dictionary_path=batches_path + '/wiki_dictionary')\n",
    "\n",
    "my_dictionary.load(dictionary_path=batches_path + '/wiki_dictionary.dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = artm.ARTM(num_topics=400,\n",
    "                  dictionary=my_dictionary,\n",
    "                  reuse_theta=False,\n",
    "                  cache_theta=False,\n",
    "                  num_document_passes=10,\n",
    "                  theta_columns_naming='title',\n",
    "                  num_processors=7,\n",
    "                  scores=[artm.PerplexityScore(name='PerplexityScore',\n",
    "                                               dictionary=my_dictionary),\n",
    "                          artm.SparsityPhiScore(name='SparsityPhiScore'),\n",
    "                          artm.SparsityThetaScore(name='SparsityThetaScore'),\n",
    "                          artm.TopicKernelScore(name='TopicKernelScore', probability_mass_threshold=0.3),\n",
    "                          artm.TopTokensScore(name='TopTokensScoreText', num_tokens=30)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.initialize(my_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# params of the online algorithm\n",
    "update_after = range(7, batch_vectorizer.num_batches, 7) + [batch_vectorizer.num_batches]\n",
    "j = np.arange(1, len(update_after) + 1)\n",
    "rho = np.power(1 + j, -0.5) \n",
    "decay_weight = 1 - rho \n",
    "apply_weight = rho * batch_vectorizer.num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# online algorithm is recommended to use 0-5 iterations\n",
    "for i in range(2):\n",
    "    model.fit_online(batch_vectorizer=batch_vectorizer, async=True,\n",
    "                     update_after=update_after, decay_weight=decay_weight, apply_weight=apply_weight)\n",
    "\n",
    "# offline algorithm is recommended to use 20-30 iterations\n",
    "for i in range(20):\n",
    "    model.fit_offline(batch_vectorizer=batch_vectorizer, num_collection_passes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Probabilistic Word Embedding for each word $w$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phibayes_phi = model.get_phi(model_name = model.model_nwt)\n",
    "pwe_vectors1 = phibayes_phi.div(phibayes_phi.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to get vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phibayes_theta = model.transform(batch_vectorizer)\n",
    "pwe_vectors2 = phibayes_theta.div(phibayes_theta.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation\n",
    "\n",
    "After fitting we can evaluate representations on a similarity task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_similar(df, positive_query_list, negative_query_list = [], top = 10):\n",
    "    vec = df.loc[positive_query_list[0]]\n",
    "    for query in positive_query_list[1:]:\n",
    "        vec *= df.loc[query]\n",
    "    for query in negative_query_list:\n",
    "        vec /= (df.loc[query])\n",
    "    sim = sklearn.metrics.pairwise.cosine_similarity(df, vec.reshape(1, -1)).flatten()\n",
    "    ind = np.argpartition(sim, -top)[-top:]\n",
    "    topind = ind[np.argsort(-sim[ind])]\n",
    "    for i in topind:\n",
    "        print df.index[i]\n",
    "    return df.index[topind[0]]\n",
    "\n",
    "\n",
    "def find_similarity(df, word1, word2):\n",
    "    vec1 = df.loc[word1]\n",
    "    vec2 = df.loc[word2]\n",
    "    return sklearn.metrics.pairwise.cosine_similarity(vec1.reshape(1, -1), vec2.reshape(1, -1))[0][0]\n",
    "\n",
    "\n",
    "def find_similarity_lk(df, word1, word2):\n",
    "    vec1 = df.loc[word1]\n",
    "    vec2 = df.loc[word2]\n",
    "    return sklearn.metrics.pairwise.linear_kernel(vec1.reshape(1, -1), vec2.reshape(1, -1))[0][0]\n",
    "\n",
    "\n",
    "def find_similarity_hellinger(df, word1, word2): #one more sqrt and division by sqrt(2) omitted, minus added\n",
    "    vec1 = df.loc[word1]\n",
    "    vec2 = df.loc[word2]\n",
    "    return -np.sum((np.sqrt(vec1) - np.sqrt(vec2)) ** 2)\n",
    "\n",
    "\n",
    "def load_human_ratings(path, df):\n",
    "    human_ratings = {}\n",
    "    added = 0\n",
    "    whole = 0\n",
    "    with codecs.open(path, encoding='utf-8') as fin:\n",
    "        for line in fin:\n",
    "            (word1, word2, sim) = line.split()\n",
    "            whole += 1\n",
    "            if word1 in df.index and word2 in df.index:\n",
    "                human_ratings[(word1, word2)] = float(sim)\n",
    "                added += 1\n",
    "    #print added, \"pairs from\", whole, \"are covered by model vocabulary and saved for evaluation.\"\n",
    "    return human_ratings\n",
    "\n",
    "\n",
    "def evaluate_sim_task(human_ratings, df, mode='none'):\n",
    "    if mode == 'cos':\n",
    "        model_ratings = {key : find_similarity(df, key[0], key[1]) for key in human_ratings}\n",
    "    elif mode == 'hellinger':\n",
    "        model_ratings = {key : find_similarity_hellinger(df, key[0], key[1]) for key in human_ratings}\n",
    "    elif mode == 'lk':\n",
    "        model_ratings = {key : find_similarity_lk(df, key[0], key[1]) for key in human_ratings}\n",
    "    else:\n",
    "        print \"Wrong mode!!!\"\n",
    "    (sorted_keys, sorted_model) = zip(*sorted(model_ratings.items(), key=lambda x: -x[1]))\n",
    "    return scipy.stats.spearmanr(sorted_model, [human_ratings[k] for k in sorted_keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list of similarity datasets \n",
    "# you can get them from hyperword script\n",
    "\n",
    "list_of_paths = ['hyperwords/testsets/ws/ws353_similarity.txt',\n",
    "                'hyperwords/testsets/ws/ws353_relatedness.txt',\n",
    "                'hyperwords/testsets/ws/ws353.txt',\n",
    "                'hyperwords/testsets/ws/bruni_men.txt',\n",
    "                'hyperwords/testsets/ws/radinsky_mturk.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for path in list_of_paths:\n",
    "    human_sims = load_human_ratings(path, phibayes)\n",
    "    key = path[-path[::-1].find('/'):-4]\n",
    "    answ = evaluate_sim_task(human_sims, pwe_vectors, mode='lk')[0]\n",
    "    print(key, answ)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
